{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlFrmwYvJH408Ke6dP/g4C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4diBk_YCfvDE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  PyTorch ecosystem\n",
        "  suits better than TensorFlow, easier to debug and make custom stuff\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn ## foundation of neural networks ->\n",
        "import torch.optim as optim # optimizers\n",
        "\n",
        "\"\"\"\n",
        "  TorchVision\n",
        "\"\"\"\n",
        "import torchvision.datasets as datasets # ready-to-go dataset\n",
        "import torchvision.transforms as transforms # preprocessing, normalization\n",
        "\n",
        "from torch.utils.data import DataLoader # standart pipeline training\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nn.Module\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\"\"\"\n",
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  __init__ function:\n",
        "  input_channels= color channels, input data is 1 color (our MNIST dataset)\n",
        "  feature_dim = how many numbers to use to describe the frame\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_channels=1, feature_dim=256):\n",
        "    super(CNNEncoder, self).__init__() # nn.Module init\n",
        "\n",
        "    \"\"\"\n",
        "    Convolutional layers\n",
        "\n",
        "      Amount of channels:\n",
        "      1 channel -> initial image\n",
        "      64 channels -> 64 different \"detectors\" (edges, corners, textures)\n",
        "      128 channels -> 128 advanced shapes\n",
        "\n",
        "      Kernel size:\n",
        "      1x1 -> precise operations (color)\n",
        "      3x3 -> local patterns (edges, small features)\n",
        "      5x5 -> wider patterns, don't need them\n",
        "\n",
        "      nn.Conv1d -> audio, text\n",
        "      nn.Conv2d -> 2d images\n",
        "      nn.Conv3d -> video, 3d models\n",
        "\n",
        "      nn.BatchNorm -> our pit-stop master, keeps our model fit and \"even\" to finish a race\n",
        "\n",
        "      nn.Linear -> our final summarizing layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.fc = nn.Linear(128, feature_dim)\n",
        "\n",
        "  \"\"\"\n",
        "  forward() pass -> the heart of neural networks\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Init input:\n",
        "      batch_size\n",
        "      1 -> because black-white images\n",
        "      28x28 -> our MNIST dataset\n",
        "      (batch_size, 1, 28, 28)\n",
        "    \"\"\"\n",
        "\n",
        "    # first block\n",
        "    x = self.conv1(x) # (batch, 1, 28, 28) → (batch, 64, 28, 28)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "     # second block\n",
        "    x = self.conv2(x) # (batch, 64, 28, 28) → (batch, 128, 28, 28)\n",
        "    x = self.bn2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    \"\"\"\n",
        "    Global Average Pooling:\n",
        "      \"суммирование\" всей карты фичей\n",
        "      усредняем все пиксели в одно число -> 28x28 = 784 convertion to 1x1 = 1\n",
        "    (batch, 128, 28, 28) → (batch, 128, 1, 1)\n",
        "    \"HUMANIZATION, sort of\"\n",
        "\n",
        "    \"\"\"\n",
        "    x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "\n",
        "    \"\"\"\n",
        "    Flatten: (batch, 128, 1, 1) → (batch, 128)\n",
        "    before flatten:\n",
        "      ┌─────┐ ┌─────┐ ┌─────┐\n",
        "      │ 0.23│ │ 0.67│ │ 0.31│ ...\n",
        "      └─────┘ └─────┘ └─────┘\n",
        "\n",
        "    after flatten:\n",
        "    0.23  0.67  0.31  0.45  0.88  ...\n",
        "\n",
        "    get ridding of extra tensors\n",
        "    \"\"\"\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # final block\n",
        "    x = self.fc(x) # (batch, 128, 28, 28) → (batch, 128)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5AR0kU4SgGxd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=1,\n",
        "                 feature_dim=256,\n",
        "                 hidden_dim=128 # our краткосрочная память, меньше cause obvious\n",
        "                 ): ## nn.Module init\n",
        "\n",
        "      super(VideoPredictor, self).__init__()\n",
        "\n",
        "      # our encoder init\n",
        "      self.cnn_encoder = CNNEncoder()\n",
        "\n",
        "      \"\"\"\n",
        "        LSTM - Long short term memory:\n",
        "          RNN model with memorization process, basically\n",
        "        The way it works:\n",
        "          1. Cell state -> long short memory\n",
        "          2. Hidden state -> \"right now\" memory\n",
        "          3. Gates:\n",
        "            3.1. Forget gate -> old information not needed, delete\n",
        "            3.2 Input gate -> new information is needed, remember\n",
        "            3.3 Output gate -> \"long story short\" output\n",
        "      \"\"\"\n",
        "      self.lstm = nn.LSTM(\n",
        "          input_size=feature_dim,\n",
        "          hidden_size=hidden_dim,\n",
        "          batch_first=True, # format\n",
        "      )\n",
        "\n",
        "      \"\"\"\n",
        "        Decoder:\n",
        "        - a painter who draws our slides\n",
        "        Steps:\n",
        "        1. Layer 1 -> expand our size to 512 to add details\n",
        "        2. Layer 2 -> add even more details\n",
        "        3. Layer 3 -> final convertion to pixels\n",
        "\n",
        "        ReLU (Rectified Linear Unit) - activation function:\n",
        "        - положительные числа -> не меняются\n",
        "        - отрицательные числа -> 0\n",
        "        - лежит в positive y,x axis -> straight line\n",
        "\n",
        "        Sigmoid - converting any number in a range of [0,1]:\n",
        "        - лежит также в positive y,x axis\n",
        "        - нужен так как пиксели должны быть в диапазоне [0,1]\n",
        "      \"\"\"\n",
        "      img_height = 64\n",
        "      img_width = 64\n",
        "\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(hidden_dim, 512), # Layer 1\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 1024), # Layer 2\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, img_height*img_width*input_channels), # Layer 3\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shapes from x\n",
        "        batch_size, seq_len, channels, height, width = x.shape\n",
        "        # print(x.shape)\n",
        "\n",
        "        # совершаем преобразование\n",
        "        x_flat = x.view(\n",
        "            batch_size * seq_len, # 5x8 = 40, instead of processing 8 videos we process 40 frames once\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "            )\n",
        "        # print(x.flat)\n",
        "\n",
        "        # cnn_encoder use we created\n",
        "        features = self.cnn_encoder(x_flat) # (batch, 128)\n",
        "\n",
        "        # doing roolback, 40 -> 8x5, because LSTM needs to know about structure\n",
        "        features = features.view(batch_size, seq_len, -1) # (8, 5, feature_dim)\n",
        "\n",
        "        \"\"\"\n",
        "        # LSTM analyzes patterns in each part of a video\n",
        "        # Input: (8, 5, feature_dim) - 8 видео по 5 фичей каждое\n",
        "        # Output: (8, 5, hidden_dim) - понимание движения для каждого шага\n",
        "        \"\"\"\n",
        "        lstm_out, (_, _) = self.lstm(features)\n",
        "\n",
        "        # LSTM is smarter than a Markov chain\n",
        "        last_step = lstm_out[:, -1, :] # yet we take last step only for predictions, (8, 5, hidden_dim) → (8, hidden_dim)\n",
        "\n",
        "        predict_flat = self.decoder(last_step) # (8, hidden_dim) → (8, 28*28)\n",
        "\n",
        "        predict_frame = predict_flat.view( # final reshape\n",
        "            batch_size,\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "        )\n",
        "\n",
        "        return predict_frame\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ry1ykLfUi5GL"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  \"\"\"\n",
        "  Moving MNIST data:\n",
        "    1. batch_size -> количество видео в батче\n",
        "    2. seq_len -> количество кадров в каждом видео\n",
        "    3. image_size -> размер кадра (image_size x image_size)\n",
        "\n",
        "  \"\"\"\n",
        "  # loading dataset\n",
        "  mnist = datasets.MNIST(\n",
        "      root='./dataset', # save folder dataset\n",
        "      train=True,\n",
        "      download=True, # if to download\n",
        "      transform=transforms.ToTensor() # transform to tensors [0,1]\n",
        "  )\n",
        "\n",
        "  return mnist\n",
        "mnist = load_dataset()"
      ],
      "metadata": {
        "id": "i8SGT_s3u4g2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_frame(batch_size=8, seq_len=5, image_size=64):\n",
        "\n",
        "  sequences = [] # all video list\n",
        "\n",
        "  for vid_idx in range(batch_size):\n",
        "\n",
        "    \"\"\"\n",
        "    Step 1: Random number\n",
        "    \"\"\"\n",
        "    digit_idx = digit_idx = np.random.randint(len(mnist)) # random index\n",
        "    digit_img, _ = mnist[digit_idx]\n",
        "    digit_np = digit_img.squeeze().numpy()\n",
        "\n",
        "    \"\"\"\n",
        "    Step 2: Initial pos and speed\n",
        "    \"\"\"\n",
        "    pos_x = np.random.randint(0,image_size - 28) # -> 0 to 36\n",
        "    pos_y = np.random.randint(0,image_size - 28) # -> 0 to 36\n",
        "\n",
        "    vel_x = np.random.randint(-2, 3) # -> -2 to 2\n",
        "    vel_y = np.random.randint(-2, 3) # -> -2 to 2\n",
        "\n",
        "    \"\"\"\n",
        "    Step 3: Our slideshow\n",
        "    \"\"\"\n",
        "    video_frames = []\n",
        "    for frame in range(seq_len+1):\n",
        "\n",
        "      frame = np.zeros((image_size, image_size), dtype=np.float16) # empty frame\n",
        "      frame[pos_y:pos_y+28, pos_x:pos_x+28] = digit_np # init pos\n",
        "      video_frames.append(frame)\n",
        "\n",
        "      pos_x += vel_x\n",
        "      pos_y += vel_y\n",
        "\n",
        "      # Bounds\n",
        "      if pos_x <= 0 or pos_x >= image_size - 28:\n",
        "          vel_x = -vel_x\n",
        "      if pos_y <= 0 or pos_y >= image_size - 28:\n",
        "          vel_y = -vel_y\n",
        "\n",
        "      # Pos correction\n",
        "      pos_x = np.clip(pos_x, 0, image_size - 28)\n",
        "      pos_y = np.clip(pos_y, 0, image_size - 28)\n",
        "\n",
        "    sequences.append(video_frames)\n",
        "\n",
        "  sequences = np.array(sequences) # преобразуем в numpy array -> удобнее для вычислений\n",
        "  sequences = np.expand_dims(sequences, axis=2) # в позиции по индексу 2 добавляем новый слот для наших channels: ( batch time  h   w) ->  (batch time, ch,  h   w)\n",
        "\n",
        "# input target separate\n",
        "  input_seq = sequences[:, :-1] # первые seq_len кадров (0-4)\n",
        "  target_seq = sequences[:, seq_len] # последний кадр (5)\n",
        "\n",
        "# преобразование в тенсоры\n",
        "  input_seq = torch.FloatTensor(input_seq)\n",
        "  target_seq = torch.FloatTensor(target_seq)\n",
        "\n",
        "  return input_seq, target_seq\n",
        "\n"
      ],
      "metadata": {
        "id": "7TKwoIRiu49t"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_create():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(f\"training on {device}\")\n",
        "\n",
        "  model = VideoPredictor(\n",
        "      input_channels=1,\n",
        "      feature_dim=256,\n",
        "      hidden_dim=128\n",
        "  )\n",
        "  model.to(device)\n",
        "  return model, device\n",
        "model, device = model_create()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "criC4DGNx4pI",
        "outputId": "759d79f3-8131-4699-9600-b8cc7e277a55"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "VideoPredictor train function\n",
        "\"\"\"\n",
        "def train_model(model=model, device=device):\n",
        "\n",
        "  \"\"\"\n",
        "  MSE - Mean Squared Error\n",
        "  loss = avg((y_pred - y_true)**2\n",
        "\n",
        "  we benefit of using MSE instead of MAE, because we use small values (working with pixels)\n",
        "  \"\"\"\n",
        "  criteria = nn.MSELoss()\n",
        "\n",
        "  \"\"\"\n",
        "  Adam (Adaptive Moment Estimation) - self-learning ball\n",
        "  - we work with basic MNIST dataset, so basic Adam will hold\n",
        "  - updating weights\n",
        "  \"\"\"\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  \"\"\"\n",
        "  specs\n",
        "  \"\"\"\n",
        "  epochs = 20\n",
        "  batch_size = 8\n",
        "  seq_len = 5\n",
        "\n",
        "  \"\"\"\n",
        "  Training process:\n",
        "  Step 1. For loop -> 20 epochs -> 20 times showing pictures to a model\n",
        "  Step 2. Each epoch generate a frame\n",
        "  Step 3. Cleaning old gradients with zero grad\n",
        "  Step 4. Forward pass and loss\n",
        "  Step 5. Backward pass and update weights\n",
        "  Step 6. Return a model\n",
        "  \"\"\"\n",
        "  for epoch in range(epochs): # 1\n",
        "    input_seq, target_seq = moving_frame(batch_size, seq_len, image_size=64) # 2\n",
        "\n",
        "    input_seq.to(device)\n",
        "    target_seq.to(device)\n",
        "\n",
        "    optimizer.zero_grad() # 3\n",
        "\n",
        "    pred = model(input_seq) # 4\n",
        "    loss = criteria(pred, target_seq.squeeze(1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      print(f'epoch [{epoch}/{epochs}], loss: {loss.item():.3f}')\n",
        "\n",
        "  print(\"Training finished\")\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "YhlXSzrmu78J"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlzqc6G_xgs7",
        "outputId": "b4edf8c9-62cf-406b-c8de-056dece08b45"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([8, 64, 64])) that is different to the input size (torch.Size([8, 1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [0/20], loss: 0.246\n",
            "epoch [5/20], loss: 0.146\n",
            "epoch [10/20], loss: 0.023\n",
            "epoch [15/20], loss: 0.020\n",
            "Training finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VideoPredictor(\n",
              "  (cnn_encoder): CNNEncoder(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (fc): Linear(in_features=128, out_features=256, bias=True)\n",
              "  )\n",
              "  (lstm): LSTM(256, 128, batch_first=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    }
  ]
}