{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNopUNiLt6b0L6S1x3oqIFA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4diBk_YCfvDE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  PyTorch ecosystem\n",
        "  suits better than TensorFlow, easier to debug and make custom stuff\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn ## foundation of neural networks ->\n",
        "import torch.optim as optim # optimizers\n",
        "\n",
        "\"\"\"\n",
        "  TorchVision\n",
        "\"\"\"\n",
        "import torchvision.datasets as datasets # ready-to-go dataset\n",
        "import torchvision.transforms as transforms # preprocessing, normalization\n",
        "\n",
        "from torch.utils.data import DataLoader # standart pipeline training\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nn.Module\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\"\"\"\n",
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  __init__ function:\n",
        "  input_channels= color channels, input data is 1 color (our MNIST dataset)\n",
        "  feature_dim = how many numbers to use to describe the frame\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_channels=1, feature_dim=512):\n",
        "    super(CNNEncoder, self).__init__() # nn.Module init\n",
        "\n",
        "    \"\"\"\n",
        "    Convolutional layers\n",
        "\n",
        "      Amount of channels:\n",
        "      1 channel -> initial image\n",
        "      64 channels -> 64 different \"detectors\" (edges, corners, textures)\n",
        "      128 channels -> 128 advanced shapes\n",
        "\n",
        "      Kernel size:\n",
        "      1x1 -> precise operations (color)\n",
        "      3x3 -> local patterns (edges, small features)\n",
        "      5x5 -> wider patterns, don't need them\n",
        "\n",
        "      nn.Conv1d -> audio, text\n",
        "      nn.Conv2d -> 2d images\n",
        "      nn.Conv3d -> video, 3d models\n",
        "\n",
        "      nn.BatchNorm -> our pit-stop master, keeps our model fit and \"even\" to finish a race\n",
        "\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "  \"\"\"\n",
        "  forward() pass -> the heart of neural networks\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Init input:\n",
        "      batch_size\n",
        "      1 -> because black-white images\n",
        "      28x28 -> our MNIST dataset\n",
        "      (batch_size, 1, 28, 28)\n",
        "    \"\"\"\n",
        "\n",
        "    # first block\n",
        "    x = self.conv1(x) # (batch, 1, 28, 28) → (batch, 64, 28, 28)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "     # second block\n",
        "    x = self.conv2(x) # (batch, 64, 28, 28) → (batch, 128, 28, 28)\n",
        "    x = self.bn2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5AR0kU4SgGxd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ry1ykLfUi5GL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}