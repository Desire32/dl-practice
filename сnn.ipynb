{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLXhEzLk2rBkaHdnoWvSc9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4diBk_YCfvDE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  PyTorch ecosystem\n",
        "  suits better than TensorFlow, easier to debug and make custom stuff\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn ## foundation of neural networks ->\n",
        "import torch.optim as optim # optimizers\n",
        "\n",
        "\"\"\"\n",
        "  TorchVision\n",
        "\"\"\"\n",
        "import torchvision.datasets as datasets # ready-to-go dataset\n",
        "import torchvision.transforms as transforms # preprocessing, normalization\n",
        "\n",
        "from torch.utils.data import DataLoader # standart pipeline training\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nn.Module\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\"\"\"\n",
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  __init__ function:\n",
        "  input_channels= color channels, input data is 1 color (our MNIST dataset)\n",
        "  feature_dim = how many numbers to use to describe the frame\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_channels=1, feature_dim=512):\n",
        "    super(CNNEncoder, self).__init__() # nn.Module init\n",
        "\n",
        "    \"\"\"\n",
        "    Convolutional layers\n",
        "\n",
        "      Amount of channels:\n",
        "      1 channel -> initial image\n",
        "      64 channels -> 64 different \"detectors\" (edges, corners, textures)\n",
        "      128 channels -> 128 advanced shapes\n",
        "\n",
        "      Kernel size:\n",
        "      1x1 -> precise operations (color)\n",
        "      3x3 -> local patterns (edges, small features)\n",
        "      5x5 -> wider patterns, don't need them\n",
        "\n",
        "      nn.Conv1d -> audio, text\n",
        "      nn.Conv2d -> 2d images\n",
        "      nn.Conv3d -> video, 3d models\n",
        "\n",
        "      nn.BatchNorm -> our pit-stop master, keeps our model fit and \"even\" to finish a race\n",
        "\n",
        "      nn.Linear -> our final summarizing layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.fc = nn.Linear(128, feature_dim)\n",
        "\n",
        "  \"\"\"\n",
        "  forward() pass -> the heart of neural networks\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Init input:\n",
        "      batch_size\n",
        "      1 -> because black-white images\n",
        "      28x28 -> our MNIST dataset\n",
        "      (batch_size, 1, 28, 28)\n",
        "    \"\"\"\n",
        "\n",
        "    # first block\n",
        "    x = self.conv1(x) # (batch, 1, 28, 28) → (batch, 64, 28, 28)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "     # second block\n",
        "    x = self.conv2(x) # (batch, 64, 28, 28) → (batch, 128, 28, 28)\n",
        "    x = self.bn2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # final block\n",
        "    x = self.fc(x) # (batch, 128, 28, 28) → (batch, 128)\n",
        "\n",
        "    \"\"\"\n",
        "    Global Average Pooling:\n",
        "      \"суммирование\" всей карты фичей\n",
        "      усредняем все пиксели в одно число -> 28x28 = 784 convertion to 1x1 = 1\n",
        "    (batch, 128, 28, 28) → (batch, 128, 1, 1)\n",
        "    \"HUMANIZATION, sort of\"\n",
        "\n",
        "    \"\"\"\n",
        "    x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "\n",
        "    \"\"\"\n",
        "    Flatten: (batch, 128, 1, 1) → (batch, 128)\n",
        "    before flatten:\n",
        "      ┌─────┐ ┌─────┐ ┌─────┐\n",
        "      │ 0.23│ │ 0.67│ │ 0.31│ ...\n",
        "      └─────┘ └─────┘ └─────┘\n",
        "\n",
        "    after flatten:\n",
        "    0.23  0.67  0.31  0.45  0.88  ...\n",
        "\n",
        "    get ridding of extra tensors\n",
        "    \"\"\"\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5AR0kU4SgGxd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=1,\n",
        "                 feature_dim=256,\n",
        "                 hidden_dim=128 # our краткосрочная память, меньше cause obvious\n",
        "                 ): ## nn.Module init\n",
        "\n",
        "      # our encoder init\n",
        "      self.cnn_encoder = CNNEncoder()\n",
        "\n",
        "      \"\"\"\n",
        "        LSTM - Long short term memory:\n",
        "          RNN model with memorization process, basically\n",
        "        The way it works:\n",
        "          1. Cell state -> long short memory\n",
        "          2. Hidden state -> \"right now\" memory\n",
        "          3. Gates:\n",
        "            3.1. Forget gate -> old information not needed, delete\n",
        "            3.2 Input gate -> new information is needed, remember\n",
        "            3.3 Output gate -> \"long story short\" output\n",
        "      \"\"\"\n",
        "      self.lstm = nn.LSTM(\n",
        "          input_size=input_channels,\n",
        "          hidden_size=hidden_dim,\n",
        "          batch_first=True, # format\n",
        "      )\n",
        "\n",
        "      \"\"\"\n",
        "        Decoder:\n",
        "        - a painter who draws our slides\n",
        "        Steps:\n",
        "        1. Layer 1 -> expand our size to 512 to add details\n",
        "        2. Layer 2 -> add even more details\n",
        "        3. Layer 3 -> final convertion to pixels\n",
        "\n",
        "        ReLU (Rectified Linear Unit) - activation function:\n",
        "        - положительные числа -> не меняются\n",
        "        - отрицательные числа -> 0\n",
        "        - лежит в positive y,x axis -> straight line\n",
        "\n",
        "        Sigmoid - converting any number in a range of [0,1]:\n",
        "        - лежит также в positive y,x axis\n",
        "        - нужен так как пиксели должны быть в диапазоне [0,1]\n",
        "      \"\"\"\n",
        "\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(hidden_dim, 512), # Layer 1\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 1024), # Layer 2\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 28*28*input_channels), # Layer 3\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "      def forward(self, x):\n",
        "        # shapes from x\n",
        "        batch_size, seq_len, channels, height, width = x.shape\n",
        "        # print(x.shape)\n",
        "\n",
        "        # совершаем преобразование\n",
        "        x_flat = x.view(\n",
        "            batch_size * seq_len, # 5x8 = 40, instead of processing 8 videos we process 40 frames once\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "            )\n",
        "        # print(x.flat)\n",
        "\n",
        "        # cnn_encoder use we created\n",
        "        features = self.cnn_encoder(x_flat) # (batch, 128)\n",
        "\n",
        "        # doing roolback, 40 -> 8x5, because LSTM needs to know about structure\n",
        "        features = features.view(batch_size, seq_len, -1) # (8, 5, feature_dim)\n",
        "\n",
        "        \"\"\"\n",
        "        # LSTM analyzes patterns in each part of a video\n",
        "        # Input: (8, 5, feature_dim) - 8 видео по 5 фичей каждое\n",
        "        # Output: (8, 5, hidden_dim) - понимание движения для каждого шага\n",
        "        \"\"\"\n",
        "        lstm_out, (_, _) = self.lstm(features)\n",
        "\n",
        "        # LSTM is smarter than a Markov chain\n",
        "        last_step = lstm_out[:, -1, :] # yet we take last step only for predictions, (8, 5, hidden_dim) → (8, hidden_dim)\n",
        "\n",
        "        predict_flat = self.decoder(last_step) # (8, hidden_dim) → (8, 28*28)\n",
        "\n",
        "        predict_frame = predict_flat.view( # final reshape\n",
        "            batch_size,\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "        )\n",
        "\n",
        "        return predict_frame\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ry1ykLfUi5GL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDS33r5ZMKXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}