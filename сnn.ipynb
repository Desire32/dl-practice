{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgOXql+zaJOoJb5Aus3Aib"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4diBk_YCfvDE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  PyTorch ecosystem\n",
        "  suits better than TensorFlow, easier to debug and make custom stuff\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn ## foundation of neural networks ->\n",
        "import torch.optim as optim # optimizers\n",
        "\n",
        "\"\"\"\n",
        "  TorchVision\n",
        "\"\"\"\n",
        "import torchvision.datasets as datasets # ready-to-go dataset\n",
        "import torchvision.transforms as transforms # preprocessing, normalization\n",
        "\n",
        "from torch.utils.data import DataLoader # standart pipeline training\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  \"\"\"\n",
        "  Moving MNIST data:\n",
        "    1. batch_size -> количество видео в батче\n",
        "    2. seq_len -> количество кадров в каждом видео\n",
        "    3. image_size -> размер кадра (image_size x image_size)\n",
        "\n",
        "  \"\"\"\n",
        "  # loading dataset\n",
        "  mnist = datasets.MNIST(\n",
        "      root='./dataset', # save folder dataset\n",
        "      train=True,\n",
        "      download=True, # if to download\n",
        "      transform=transforms.ToTensor() # transform to tensors [0,1]\n",
        "  )\n",
        "\n",
        "  return mnist\n",
        "mnist = load_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq2k0zo1ViNU",
        "outputId": "ba3d064d-b6e2-46b8-89fa-b932a4b059f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 488kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.41MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.71MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_frames(batch_size=8, seq_len=5, image_size=64):\n",
        "\n",
        "  sequences = [] # all video list\n",
        "\n",
        "  for vid_idx in range(batch_size):\n",
        "\n",
        "    \"\"\"\n",
        "    Step 1: Random number\n",
        "    \"\"\"\n",
        "    digit_idx = digit_idx = np.random.randint(len(mnist)) # random index\n",
        "    digit_img, _ = mnist[digit_idx]\n",
        "    digit_np = digit_img.squeeze().numpy()\n",
        "\n",
        "    \"\"\"\n",
        "    Step 2: Initial pos and speed\n",
        "    \"\"\"\n",
        "    pos_x = np.random.randint(0,image_size - 28) # -> 0 to 36\n",
        "    pos_y = np.random.randint(0,image_size - 28) # -> 0 to 36\n",
        "\n",
        "    vel_x = np.random.randint(-2, 3) # -> -2 to 2\n",
        "    vel_y = np.random.randint(-2, 3) # -> -2 to 2\n",
        "\n",
        "    \"\"\"\n",
        "    Step 3: Our slideshow\n",
        "    \"\"\"\n",
        "    video_frames = []\n",
        "    for frame in range(seq_len+1):\n",
        "\n",
        "      frame = np.zeros(image_size, image_size) # empty frame\n",
        "      frame[pos_y:pos_y+28, pos_x:pos_x+28] = digit_np # init pos\n",
        "      video_frames.append(frame)\n",
        "\n",
        "      pos_x += vel_x\n",
        "      pos_y += vel_y\n",
        "\n",
        "      # Bounds\n",
        "      if pos_x <= 0 or pos_x >= image_size - 28:\n",
        "          vel_x = -vel_x\n",
        "      if pos_y <= 0 or pos_y >= image_size - 28:\n",
        "          vel_y = -vel_y\n",
        "\n",
        "      # Pos correction\n",
        "      pos_x = np.clip(pos_x, 0, image_size - 28)\n",
        "      pos_y = np.clip(pos_y, 0, image_size - 28)\n",
        "\n",
        "    sequences.append(video_frames)\n",
        "\n",
        "  sequences = np.array(sequences) # преобразуем в numpy array -> удобнее для вычислений\n",
        "  sequences = np.expand_dims(sequences, axis=2) # в позиции по индексу 2 добавляем новый слот для наших channels: ( batch time  h   w) ->  (batch time, ch,  h   w)\n",
        "\n",
        "# input target separate\n",
        "  input_seq = sequences[:, :-1] # первые seq_len кадров (0-4)\n",
        "  target_seq = sequences[:, 1:] # последний кадр (5)\n",
        "\n",
        "# преобразование в тенсоры\n",
        "  input_seq = torch.FloatTensor(input_seq)\n",
        "  target_seq = torch.FloatTensor(target_seq)\n",
        "\n",
        "  return input_seq, target_seq\n"
      ],
      "metadata": {
        "id": "rRDfw4tZSdFe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nn.Module\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\"\"\"\n",
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  __init__ function:\n",
        "  input_channels= color channels, input data is 1 color (our MNIST dataset)\n",
        "  feature_dim = how many numbers to use to describe the frame\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_channels=1, feature_dim=512):\n",
        "    super(CNNEncoder, self).__init__() # nn.Module init\n",
        "\n",
        "    \"\"\"\n",
        "    Convolutional layers\n",
        "\n",
        "      Amount of channels:\n",
        "      1 channel -> initial image\n",
        "      64 channels -> 64 different \"detectors\" (edges, corners, textures)\n",
        "      128 channels -> 128 advanced shapes\n",
        "\n",
        "      Kernel size:\n",
        "      1x1 -> precise operations (color)\n",
        "      3x3 -> local patterns (edges, small features)\n",
        "      5x5 -> wider patterns, don't need them\n",
        "\n",
        "      nn.Conv1d -> audio, text\n",
        "      nn.Conv2d -> 2d images\n",
        "      nn.Conv3d -> video, 3d models\n",
        "\n",
        "      nn.BatchNorm -> our pit-stop master, keeps our model fit and \"even\" to finish a race\n",
        "\n",
        "      nn.Linear -> our final summarizing layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.fc = nn.Linear(128, feature_dim)\n",
        "\n",
        "  \"\"\"\n",
        "  forward() pass -> the heart of neural networks\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Init input:\n",
        "      batch_size\n",
        "      1 -> because black-white images\n",
        "      28x28 -> our MNIST dataset\n",
        "      (batch_size, 1, 28, 28)\n",
        "    \"\"\"\n",
        "\n",
        "    # first block\n",
        "    x = self.conv1(x) # (batch, 1, 28, 28) → (batch, 64, 28, 28)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "     # second block\n",
        "    x = self.conv2(x) # (batch, 64, 28, 28) → (batch, 128, 28, 28)\n",
        "    x = self.bn2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # final block\n",
        "    x = self.fc(x) # (batch, 128, 28, 28) → (batch, 128)\n",
        "\n",
        "    \"\"\"\n",
        "    Global Average Pooling:\n",
        "      \"суммирование\" всей карты фичей\n",
        "      усредняем все пиксели в одно число -> 28x28 = 784 convertion to 1x1 = 1\n",
        "    (batch, 128, 28, 28) → (batch, 128, 1, 1)\n",
        "    \"HUMANIZATION, sort of\"\n",
        "\n",
        "    \"\"\"\n",
        "    x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "\n",
        "    \"\"\"\n",
        "    Flatten: (batch, 128, 1, 1) → (batch, 128)\n",
        "    before flatten:\n",
        "      ┌─────┐ ┌─────┐ ┌─────┐\n",
        "      │ 0.23│ │ 0.67│ │ 0.31│ ...\n",
        "      └─────┘ └─────┘ └─────┘\n",
        "\n",
        "    after flatten:\n",
        "    0.23  0.67  0.31  0.45  0.88  ...\n",
        "\n",
        "    get ridding of extra tensors\n",
        "    \"\"\"\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5AR0kU4SgGxd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=1,\n",
        "                 feature_dim=256,\n",
        "                 hidden_dim=128 # our краткосрочная память, меньше cause obvious\n",
        "                 ): ## nn.Module init\n",
        "\n",
        "      # our encoder init\n",
        "      self.cnn_encoder = CNNEncoder()\n",
        "\n",
        "      \"\"\"\n",
        "        LSTM - Long short term memory:\n",
        "          RNN model with memorization process, basically\n",
        "        The way it works:\n",
        "          1. Cell state -> long short memory\n",
        "          2. Hidden state -> \"right now\" memory\n",
        "          3. Gates:\n",
        "            3.1. Forget gate -> old information not needed, delete\n",
        "            3.2 Input gate -> new information is needed, remember\n",
        "            3.3 Output gate -> \"long story short\" output\n",
        "      \"\"\"\n",
        "      self.lstm = nn.LSTM(\n",
        "          input_size=input_channels,\n",
        "          hidden_size=hidden_dim,\n",
        "          batch_first=True, # format\n",
        "      )\n",
        "\n",
        "      \"\"\"\n",
        "        Decoder:\n",
        "        - a painter who draws our slides\n",
        "        Steps:\n",
        "        1. Layer 1 -> expand our size to 512 to add details\n",
        "        2. Layer 2 -> add even more details\n",
        "        3. Layer 3 -> final convertion to pixels\n",
        "\n",
        "        ReLU (Rectified Linear Unit) - activation function:\n",
        "        - положительные числа -> не меняются\n",
        "        - отрицательные числа -> 0\n",
        "        - лежит в positive y,x axis -> straight line\n",
        "\n",
        "        Sigmoid - converting any number in a range of [0,1]:\n",
        "        - лежит также в positive y,x axis\n",
        "        - нужен так как пиксели должны быть в диапазоне [0,1]\n",
        "      \"\"\"\n",
        "\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(hidden_dim, 512), # Layer 1\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 1024), # Layer 2\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 28*28*input_channels), # Layer 3\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "      def forward(self, x):\n",
        "        # shapes from x\n",
        "        batch_size, seq_len, channels, height, width = x.shape\n",
        "        # print(x.shape)\n",
        "\n",
        "        # совершаем преобразование\n",
        "        x_flat = x.view(\n",
        "            batch_size * seq_len, # 5x8 = 40, instead of processing 8 videos we process 40 frames once\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "            )\n",
        "        # print(x.flat)\n",
        "\n",
        "        # cnn_encoder use we created\n",
        "        features = self.cnn_encoder(x_flat) # (batch, 128)\n",
        "\n",
        "        # doing roolback, 40 -> 8x5, because LSTM needs to know about structure\n",
        "        features = features.view(batch_size, seq_len, -1) # (8, 5, feature_dim)\n",
        "\n",
        "        \"\"\"\n",
        "        # LSTM analyzes patterns in each part of a video\n",
        "        # Input: (8, 5, feature_dim) - 8 видео по 5 фичей каждое\n",
        "        # Output: (8, 5, hidden_dim) - понимание движения для каждого шага\n",
        "        \"\"\"\n",
        "        lstm_out, (_, _) = self.lstm(features)\n",
        "\n",
        "        # LSTM is smarter than a Markov chain\n",
        "        last_step = lstm_out[:, -1, :] # yet we take last step only for predictions, (8, 5, hidden_dim) → (8, hidden_dim)\n",
        "\n",
        "        predict_flat = self.decoder(last_step) # (8, hidden_dim) → (8, 28*28)\n",
        "\n",
        "        predict_frame = predict_flat.view( # final reshape\n",
        "            batch_size,\n",
        "            channels,\n",
        "            height,\n",
        "            width\n",
        "        )\n",
        "\n",
        "        return predict_frame\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ry1ykLfUi5GL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDS33r5ZMKXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}